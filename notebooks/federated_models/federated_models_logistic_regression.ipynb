{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated models: logistic regression\n",
    "\n",
    "Here, we explain how to set up a federated classification experiment using a Logistic Regression model.\n",
    "Results from the federated learning are compared to the (non-federated) centralized learning.\n",
    "Moreover, we also show how the addition of differential privacy affects the performance of the Federated model. \n",
    "In these examples, we will generate synthetic data for the classification task. \n",
    "In particular, we will start from a two-dimensional case, since with only two features, we are able to easily plot the samples and the decision boundaries computed by the classifier. \n",
    "After that, we will repeat the experiment by adding more features and classes to the synthetic database.\n",
    "\n",
    "## The data\n",
    "We generate the data using the `make_classification` function from [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shfl\n",
    "from shfl.data_base.data_base import LabeledDatabase\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from shfl.private.reproducibility import Reproducibility\n",
    "\n",
    "# Comment to turn off reproducibility:\n",
    "Reproducibility(1234)\n",
    "\n",
    "# Create database:\n",
    "n_features = 2\n",
    "n_classes = 3\n",
    "n_samples = 500\n",
    "data, labels = make_classification(\n",
    "    n_samples=n_samples, n_features=n_features, n_informative=2, \n",
    "    n_redundant=0, n_repeated=0, n_classes=n_classes, \n",
    "    n_clusters_per_class=1, weights=None, flip_y=0.1, class_sep=0.4, random_state=1234)\n",
    "database = LabeledDatabase(data, labels)\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = database.load_data()\n",
    "\n",
    "print(\"Shape of training and test data: \" + str(train_data.shape) + str(test_data.shape))\n",
    "print(\"Shape of training and test labels: \" + str(train_labels.shape) + str(test_labels.shape))\n",
    "print(train_data[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, in this two-feature case, it is beneficial to visualize the results. \n",
    "For that purpose, we will use the following function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_2D_decision_boundary(model, data, labels, title=None):\n",
    "    # Step size of the mesh. The smaller it is, better the quality\n",
    "    h = .02 \n",
    "    # Color map\n",
    "    cmap = plt.cm.Set1\n",
    "    \n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Obtain labels for each point in mesh. Use last trained model.\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    fig, ax = plt.subplots(figsize=(9,6))\n",
    "    plt.clf()\n",
    "    plt.imshow(Z, interpolation='nearest',\n",
    "               extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "               cmap=cmap,\n",
    "               alpha=0.6,\n",
    "               aspect='auto', origin='lower')\n",
    "    # Plot data:\n",
    "    plt.scatter(data[:, 0], data[:, 1], c=labels, cmap=cmap, s=40, marker='o')\n",
    "    \n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel('Feature 1', fontsize=18)\n",
    "    plt.ylabel('Feature 2', fontsize=18)\n",
    "    plt.tick_params(labelsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "The Sherpa.ai Federated Learning and Differential Privacy Framework offers support for the Logistic Regression model from [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). The user must specify, in advance, the number of features and the target classes: the assumption for this Federated Logistic Regression example is that each client's data possesses at least one sample of each class.\n",
    "Otherwise, each node might train a different classification problem, and it would be problematic to aggregate the global model.\n",
    "Setting a model's state parameter to `warm_start:True` tells the clients to restart the training from the Federated round update.\n",
    "To assess the performance, we compute the *Balanced Accuracy* and the *Kohen Kappa* scores (see [metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from shfl.model.linear_classifier_model import LinearClassifierModel\n",
    "\n",
    "# Define the model:\n",
    "classes = np.unique(train_labels)\n",
    "def model_builder():\n",
    "    sk_model = LogisticRegression(warm_start=True, solver='lbfgs', multi_class='auto')\n",
    "    model = LinearClassifierModel(n_features=n_features, classes=classes, model=sk_model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train the model on centralized data (i.e. non-federated), which will be our reference model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on centralized data for comparison:\n",
    "model_centralized = LinearClassifierModel(n_features=n_features, classes=classes)\n",
    "model_centralized.train(train_data, train_labels)\n",
    "print('Centralized test performance: ' + str(model_centralized.evaluate(test_data, test_labels)))\n",
    "\n",
    "# Plot decision boundaries and test data for the centralized (non-Federated) case:\n",
    "if n_features == 2:\n",
    "    plot_2D_decision_boundary(model_centralized._model, test_data, test_labels, title = \"Centralized Logistic regression. Test data are shown.\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the federated learning experiment\n",
    "We are ready to run our model in a federated configuration. \n",
    "We distribute the data over the nodes, assuming the data is IID.\n",
    "Next, we define the aggregation of the federated outputs to be the average of the client models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribute data over the nodes:\n",
    "iid_distribution = shfl.data_distribution.IidDataDistribution(database)\n",
    "n_clients = 5\n",
    "federated_data, test_data, test_labels = iid_distribution.get_federated_data(num_nodes=n_clients, percent=100)\n",
    "aggregator = shfl.federated_aggregator.FedAvgAggregator()\n",
    "\n",
    "\n",
    "# Run the federated experiment:\n",
    "federated_government = shfl.federated_government.FederatedGovernment(model_builder, federated_data, aggregator)\n",
    "federated_government.run_rounds(n=2, test_data=test_data, test_label=test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that the performance of the federated global model is generally superior with respect to the performance of each node, thus, the federated learning approach proves to be beneficial.\n",
    "Moreover, since no or little performance difference is observed between the federated rounds, we can conclude that the classification problem converges very early, in this setting, and no further rounds are required. This might be due to the IID nature of the client data when performing classification: each node gets a representative chunk of data and thus the local models are similar.\n",
    "\n",
    "\n",
    "It can be observed that the performance of federated global model is comparable to the performance of the model trained on centralized data, and it produces similar decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundaries and test data for the Federated case:\n",
    "if n_features == 2:\n",
    "    plot_2D_decision_boundary(federated_government.global_model, test_data, test_labels, title = \"Federated Logistic regression. Test data are shown.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add differential privacy \n",
    "We want to assess the impact of differential privacy (see [The Algorithmic Foundations of Differential Privacy](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf), Section 3.3) on the federated model's performance. \n",
    "### Model's sensitivity\n",
    "In particular, we will use the Laplace mechanism (see also the corresponding [Laplace mechanism notebook](../differential_privacy/differential_privacy_laplace.ipynb)). \n",
    "The noise added has to be of the same order as the sensitivity of the model's output, i.e., the model parameters of our logistic regression. \n",
    "In general, the sensitivity of a Machine Learning model is difficult to compute (for the Logistic Regression case, refer to [Privacy-preserving logistic regression](http://papers.nips.cc/paper/3486-privacy-preserving-logistic-regression.pdf)). \n",
    "An alternative strategy may be to estimate the sensitivity through a sampling procedure (e.g., see [Rubinstein 2017](https://arxiv.org/abs/1706.02562) and how to use the tools provided by Sherpa.ai Federated Learning and Differential Privacy Framework in the [Linear Regression Notebook](../federated_models/federated_models_linear_regression.ipynb)). \n",
    "However, be advised that this would guarantee the weaker property of random differential privacy.\n",
    "This approach is convenient, since it allows for the sensitivity estimation of an arbitrary model or a black-box computer function.\n",
    "The Sherpa.ai Federated Learning and Differential Privacy Framework provides this functionality in the class `SensitivitySampler`.\n",
    "\n",
    "\n",
    "We need to specify a distribution of the data to sample from. Generally, this requires previous knowledge and/or model assumptions. \n",
    "In order not to make any specific assumptions about the distribution of the dataset, we can choose a uniform distribution. \n",
    "We define our class of `ProbabilityDistribution` that uniformly samples over a data-frame.\n",
    "We could sample using the training data. However, since in a real case, the training data would be the actual client data, we wouldn't have access to it. \n",
    "Thus, we generate another synthetic dataset for sampling (in a real case, this could be a public database we are able to access):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformDistribution(shfl.differential_privacy.ProbabilityDistribution):\n",
    "    \"\"\"\n",
    "    Implement Uniform sampling over the data\n",
    "    \"\"\"\n",
    "    def __init__(self, sample_data):\n",
    "        self._sample_data = sample_data\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        row_indices = np.random.randint(low=0, high=self._sample_data.shape[0], size=sample_size, dtype='l')\n",
    "        \n",
    "        return self._sample_data[row_indices, :]\n",
    "    \n",
    "# Create sampling database:\n",
    "n_samples = 150\n",
    "sampling_data, sampling_labels = make_classification(\n",
    "    n_samples=n_samples, n_features=n_features, n_informative=2, \n",
    "    n_redundant=0, n_repeated=0, n_classes=n_classes, \n",
    "    n_clusters_per_class=1, weights=None, flip_y=0.1, class_sep=0.1)    \n",
    "sample_data = np.hstack((sampling_data, sampling_labels.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `SensitivitySampler` implements the sampling, given a query (i.e., the learning model itself, in this case).\n",
    "We only need to add the `get` method to our model since it is required by the class `SensitivitySampler`: it simply trains the model on the input data and outputs the trained parameters. \n",
    "We choose the sensitivity norm to be the $L_1$ norm and we apply the sampling. \n",
    "The value of the sensitivity depends on the number of samples `n`: the more samples we perform, the more accurate the sensitivity. \n",
    "Indeed, by increasing the number of samples `n`, the sensitivity becomes more accurate and typically decreases.\n",
    "Note that the sampling could be quite costly, since the query (i.e. the model, in this case) is called, each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.differential_privacy import SensitivitySampler\n",
    "from shfl.differential_privacy import L1SensitivityNorm\n",
    "\n",
    "class LogisticRegressionSample(LinearClassifierModel):\n",
    "    \n",
    "    def get(self, data_array):\n",
    "        data = data_array[:, 0:-1]\n",
    "        labels = data_array[:, -1]\n",
    "        train_model = self.train(data, labels)\n",
    "      \n",
    "        return self.get_model_params()\n",
    "\n",
    "distribution = UniformDistribution(sample_data)\n",
    "sampler = SensitivitySampler()\n",
    "\n",
    "n_samples = 200\n",
    "max_sensitivity, mean_sensitivity = sampler.sample_sensitivity(\n",
    "    LogisticRegressionSample(n_features=n_features, classes=classes), \n",
    "    L1SensitivityNorm(), distribution, n=n_samples, gamma=0.05)\n",
    "print(\"Max sensitivity from sampling: \" + str(max_sensitivity))\n",
    "print(\"Mean sensitivity from sampling: \" + str(mean_sensitivity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the federated learning experiment with differential privacy\n",
    "Once the model's estimated sensitivity has been obtained, we fix the $\\epsilon$ privacy budget and we can run the privacy-preserving Federated experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.differential_privacy import LaplaceMechanism\n",
    "\n",
    "params_access_definition = LaplaceMechanism(sensitivity=max_sensitivity, epsilon=0.5)\n",
    "federated_governmentDP = shfl.federated_government.FederatedGovernment(\n",
    "    model_builder, federated_data, aggregator, model_params_access=params_access_definition)\n",
    "\n",
    "federated_governmentDP.run_rounds(n=2, test_data=test_data, test_label=test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect, the addition of random noise slightly alters the solution, but the model is still comparable to the non-private federated case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundaries and test data for Privacy-preserving Federated case:\n",
    "if n_features == 2:\n",
    "    plot_2D_decision_boundary(federated_governmentDP.global_model, test_data, test_labels,\n",
    "                              title = \"Privacy-preserving Federated Logistic regression. Test data are shown.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note 1:** In this case, we only run a few federated rounds. However, in general, you should make sure not to exceed a fixed privacy budget (see how to achieve that  in *Sherpa.ai Federated Learning Framework* in the [Composition concepts notebook](../differential_privacy/differential_privacy_composition_concepts.ipynb)).   \n",
    "**Note 2:** It is worth mentioning that the above results cannot be considered general. Some factors that considerably influence the classification problem are, for example, the training dataset, the model type, and the differential privacy mechanism used. \n",
    "In fact, the classification problem itself depends on the training data (number of features, whether the classes are separable or not etc.). \n",
    "We strongly encourage users to play with the values for generating the database (such as `n_features, n_classes, n_samples, class_sep ...` see their meaning [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification)), or try different classification datasets, since convergence and accuracy of local and global models can be strongly affected. \n",
    "Moreover, even without changing the dataset, by running the present experiment multiple times (you need to comment the Reproducibility command line code), it is observed that the federated global model may also exhibit slightly better performance, when compared to the centralized model (we use `random_state` input, in order to always produce the same dataset). \n",
    "Similarly, the privacy-preserving federate model might exhibit even better performance, compared to the non-private version. \n",
    "This depends on a) the performance metrics chosen and b) the idiosyncrasy of the specific classification considered here: a small modification to the model's coefficients may alter the class prediction for a few samples.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case with more features and classes: \n",
    "Below we present a more complex case, introducing more features and classes. When using more than two features, the figures are not plotted. Since the structure of the example is identical to the above, the comments are not repeated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database:\n",
    "n_features = 11\n",
    "n_classes = 5\n",
    "n_samples = 500\n",
    "data, labels = make_classification(\n",
    "    n_samples=n_samples, n_features=n_features, n_informative=4, \n",
    "    n_redundant=0, n_repeated=0, n_classes=n_classes, \n",
    "    n_clusters_per_class=2, weights=None, flip_y=0.1, class_sep=0.1, random_state=123)\n",
    "database = LabeledDatabase(data, labels)\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = database.load_data()\n",
    "print(\"Shape of training and test data: \" + str(train_data.shape) + str(test_data.shape))\n",
    "print(\"Shape of training and test labels: \" + str(train_labels.shape) + str(test_labels.shape))\n",
    "print(train_data[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the federated learning experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iid_distribution = shfl.data_distribution.IidDataDistribution(database)\n",
    "federated_data, test_data, test_labels = iid_distribution.get_federated_data(num_nodes=5, percent=100)\n",
    "\n",
    "classes = np.unique(train_labels)\n",
    "def model_builder():\n",
    "    sk_model = LogisticRegression(warm_start=True, solver='lbfgs', multi_class='auto')\n",
    "    model = LinearClassifierModel(n_features=n_features, classes=classes, model=sk_model)\n",
    "    return model\n",
    "\n",
    "aggregator = shfl.federated_aggregator.FedAvgAggregator()\n",
    "\n",
    "federated_government = shfl.federated_government.FederatedGovernment(model_builder, federated_data, aggregator)\n",
    "federated_government.run_rounds(n=2, test_data=test_data, test_label=test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison with centralized training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on centralized data:\n",
    "model_centralized = LinearClassifierModel(n_features=n_features, classes=classes)\n",
    "model_centralized.train(train_data, train_labels)\n",
    "if n_features == 2:\n",
    "    plot_2D_decision_boundary(model_centralized._model, train_data, train_labels, title = \"Benchmark: Logistic regression using Centralized data\")\n",
    "print('Centralized test performance: ' + str(model_centralized.evaluate(test_data, test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the federated learning experiment with differential privacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sampling database:\n",
    "n_samples = 500\n",
    "sampling_data, sampling_labels = make_classification(\n",
    "    n_samples=n_samples, n_features=n_features, n_informative=4, \n",
    "    n_redundant=0, n_repeated=0, n_classes=n_classes, \n",
    "    n_clusters_per_class=2, weights=None, flip_y=0.1, class_sep=0.1, random_state=123)   \n",
    "sample_data = np.hstack((sampling_data, sampling_labels.reshape(-1,1)))\n",
    "distribution = UniformDistribution(sample_data)\n",
    "\n",
    "# Sample sensitivity:\n",
    "n_samples = 300\n",
    "max_sensitivity, mean_sensitivity = sampler.sample_sensitivity(\n",
    "    LogisticRegressionSample(n_features=n_features, classes=classes), \n",
    "    L1SensitivityNorm(), distribution, n=n_samples, gamma=0.05)\n",
    "print(\"Max sensitivity from sampling: \" + str(max_sensitivity))\n",
    "print(\"Mean sensitivity from sampling: \" + str(mean_sensitivity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.differential_privacy import LaplaceMechanism\n",
    "\n",
    "params_access_definition = LaplaceMechanism(sensitivity=max_sensitivity, epsilon=0.5)\n",
    "federated_governmentDP = shfl.federated_government.FederatedGovernment(\n",
    "    model_builder, federated_data, aggregator, model_params_access=params_access_definition)\n",
    "\n",
    "federated_governmentDP.run_rounds(n=2, test_data=test_data, test_label=test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
