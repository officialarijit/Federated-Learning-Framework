{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated models: non-linear kernel SVM classification\n",
    "\n",
    "In the present notebook, the idea for a Federated non-linear support vector machine (SVM) classification is presented. \n",
    "The model is encapsulated in the Sherpa.ai Federated Learning Framework on for a synthetic database.\n",
    "Moreover, differential privacy is applied and its impact on the global model is assessed. \n",
    "\n",
    "## The data\n",
    "We start by creating a synthetic database: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shfl\n",
    "from shfl.data_base.data_base import LabeledDatabase\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from shfl.model.linear_classifier_model import LinearClassifierModel\n",
    "import numpy as np\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "# Create database:\n",
    "n_features = 2\n",
    "n_classes = 3\n",
    "data, labels = make_classification(\n",
    "    n_samples=1000, n_features=n_features, n_informative=2, \n",
    "    n_redundant=0, n_repeated=0, n_classes=n_classes, \n",
    "    n_clusters_per_class=1, weights=None, flip_y=0.1, class_sep=0.5)\n",
    "database = LabeledDatabase(data, labels)\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = database.load_data()\n",
    "\n",
    "# Model params if using NuSVC:\n",
    "#nu = 0.7 \n",
    "#kwargs = {'nu':nu}\n",
    "#model_use = NuSVC(**kwargs)\n",
    "\n",
    "# Model params if using SVC:\n",
    "C = 1\n",
    "kwargs = {'C':C}\n",
    "model_use = SVC(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of train and test data: \" + str(train_data.shape) + str(test_data.shape))\n",
    "print(\"Shape of train and test labels: \" + str(train_labels.shape) + str(test_labels.shape))\n",
    "print(train_data[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "Next, we define the new class for SVM using [`sklearn`'s Support Vector Machine classifiers](https://scikit-learn.org/stable/modules/svm.html).\n",
    "By the implementation below, you can use either `SVC` or `NuSVC`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMClassifierModel(LinearClassifierModel):\n",
    "    \"\"\"\n",
    "    This class offers support for scikit-learn SVM non-linear classification. It implements [TrainableModel](../Model/#trainablemodel-class)\n",
    "\n",
    "    # Arguments:\n",
    "        n_features: integer number of features (independent variables).\n",
    "        classes: array of classes to predict. At least 2 classes must be provided.\n",
    "        model: initialized model to employ. Options are [SVC (default)](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) \n",
    "        and [NuSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, classes, model=None):\n",
    "        if model is None:\n",
    "            model = SVC()\n",
    "        self._check_initialization(n_features, classes)\n",
    "        self._model = model\n",
    "        self._n_features = n_features\n",
    "        classes = np.sort(np.asarray(classes))\n",
    "        self._model.classes_ = classes\n",
    "        params = np.array([], dtype=np.int64).reshape(0, (n_features + 1))\n",
    "        self.set_model_params(params)\n",
    "        \n",
    "    def train(self, data, labels):\n",
    "        \"\"\"\n",
    "        Implementation of abstract method of class [TrainableModel](../Model/#trainablemodel-class)\n",
    "\n",
    "        # Arguments\n",
    "            data: Data, array-like of shape (n_samples, n_features)\n",
    "            labels: Target classes, array-like of shape (n_samples,) \n",
    "        \"\"\"\n",
    "        \n",
    "        data_array = np.row_stack((self.get_model_params(), \n",
    "                                   np.column_stack((data, labels)) ))\n",
    "        data_array_unq, count = np.unique(data_array, axis=0, return_counts=True)\n",
    "        data = data_array_unq[:, 0:-1]\n",
    "        labels = data_array_unq[:, -1]\n",
    "        \n",
    "        self._check_data(data)\n",
    "        self._check_labels_train(labels)\n",
    "        self._model.fit(data, labels)\n",
    "\n",
    "    def get_model_params(self):\n",
    "        \"\"\"\n",
    "        Implementation of abstract method of class [TrainableModel](../Model/#trainablemodel-class)\n",
    "        \"\"\"\n",
    "        classes_index = [np.full(i_nSV, i_class) for (i_class, i_nSV) in \n",
    "                  zip(range(len(self._model.classes_)), self._model.n_support_)]\n",
    "        classes_index = np.hstack(classes_index)\n",
    "        params = np.column_stack((self._model.support_vectors_, classes_index))\n",
    "        params = params.astype(self._model.support_vectors_.dtype)\n",
    "        \n",
    "        return params\n",
    "\n",
    "    def set_model_params(self, params):\n",
    "        \"\"\"\n",
    "        Implementation of abstract method of class [TrainableModel](../Model/#trainablemodel-class)\n",
    "        \"\"\"\n",
    "        self._model.support_vectors_ = np.ascontiguousarray(params[:, 0:-1], dtype=params.dtype)\n",
    "        labels = params[:,-1]\n",
    "        n_support = [np.sum(labels == i_class) for i_class in range(len(self._model.classes_))]\n",
    "        self._model._n_support = np.asarray(n_support, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As reference, we train a centralized model (i.e. non-federated). And in the case of two features, we can visualize the solution on a plane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_2D_decision_boundary(model, data, labels, title=None):\n",
    "    # Step size of the mesh. Smaller it is, better the quality\n",
    "    h = .02 \n",
    "    # Color map\n",
    "    cmap = plt.cm.Set1\n",
    "    \n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Obtain labels for each point in mesh. Use last trained model.\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    fig, ax = plt.subplots(figsize=(9,6))\n",
    "    plt.clf()\n",
    "    plt.imshow(Z, interpolation='nearest',\n",
    "               extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "               cmap=cmap,\n",
    "               alpha=0.6,\n",
    "               aspect='auto', origin='lower')\n",
    "    # Plot data:\n",
    "    plt.scatter(data[:, 0], data[:, 1], c=labels, cmap=cmap, s=40, marker='o')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train global model using framework's class: \n",
    "classes = [i for i in range(0,n_classes)] \n",
    "model_centralized = SVMClassifierModel(n_features=n_features, classes=classes, model=model_use)\n",
    "model_centralized.train(data=train_data, labels=train_labels)\n",
    "if n_features == 2:\n",
    "    plot_2D_decision_boundary(model_centralized, test_data, labels=test_labels, title = \"Benchmark: Classification using Centralized data\")\n",
    "print(\"Test performance using centralized data: \" + str(model_centralized.evaluate(test_data, test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to aggregate a model's parameters from each federated node\n",
    "`sklearn` provides three options for classification: `LinearSVC`, `SVC` and `NuSVC`. \n",
    "The linear version `LinearSVC` is easily incorporated in the platform, since the aggregation of the model is straightforward (see [notebook for Federated Logistic Regression](./federated_models_logistic_regression.ipynb), in which `LinearSVC` can be used instead of the logistic regression). \n",
    "On the other hand, for `SVC` and `NuSVC`, the output model's parameters are more complex, since they depend on the number of support vectors for each class. \n",
    "Thus, in principle, each client would deliver parameters with different dimensions, which are not straightforward to aggregate. \n",
    "\n",
    "Here, we use the clients' support vectors to *train a global model directly on the server*, obtaining the aggregated model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import inspect\n",
    "from shfl.federated_aggregator.federated_aggregator import FederatedAggregator\n",
    "\n",
    "class GlobalModelAggregator(FederatedAggregator):\n",
    "    \"\"\"\n",
    "    Trains the global model over the local parameters to aggregate them.\n",
    "\n",
    "    It implements [Federated Aggregator](../federated_aggregator/#federatedaggregator-class)\n",
    "    \"\"\"\n",
    "\n",
    "    def aggregate_weights(self, clients_params):\n",
    "        \"\"\"\n",
    "        Implementation of abstract method of class [AggregateWeightsFunction](../federated_aggregator/#federatedaggregator-class)\n",
    "        # Arguments:\n",
    "            clients_params: list of multi-dimensional (numeric) arrays. Each entry in the list contains the model's parameters of one client.\n",
    "\n",
    "        # Returns\n",
    "            aggregated_weights: aggregator weights representing the global learning model\n",
    "\n",
    "        # References\n",
    "            [Communication-Efficient Learning of Deep Networks from Decentralized Data](https://arxiv.org/abs/1602.05629)\n",
    "        \"\"\"\n",
    "        caller_object = inspect.currentframe().f_back.f_locals['self']\n",
    "        clients_params_array = np.vstack(clients_params)\n",
    "        caller_object.global_model.train(clients_params_array[:, 0:-1], clients_params_array[:, -1].astype(int))\n",
    "\n",
    "        return caller_object.global_model.get_model_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the federated learning experiment\n",
    "Once defined the aggregator, we can run the federated model.\n",
    "Note that the decision boundary can vary even by running the training on the same data (this is due to the internal shuffle of the data of the SVM solver). \n",
    "Thus, in order to compare the centralized and the federated models, it is more relevant to compare the scores on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iid_distribution = shfl.data_distribution.IidDataDistribution(database)\n",
    "federated_data, test_data, test_labels = iid_distribution.get_federated_data(num_nodes=5, percent=100)\n",
    "\n",
    "classes = [i for i in range(0,n_classes)] \n",
    "def model_builder():\n",
    "    model = SVMClassifierModel(n_features=n_features, classes=classes, model=model_use)\n",
    "    return model\n",
    "\n",
    "aggregator = GlobalModelAggregator()\n",
    "\n",
    "federated_government = shfl.federated_government.FederatedGovernment(model_builder, federated_data, aggregator)\n",
    "federated_government.run_rounds(n=3, test_data=test_data, test_label=test_labels)\n",
    "\n",
    "if n_features == 2:\n",
    "    plot_2D_decision_boundary(federated_government.global_model, test_data, test_labels, title = \"Global model: Classification using Federated data\")\n",
    "\n",
    "print(\"Model used: \" + type(federated_government.global_model._model).__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add differential privacy\n",
    "\n",
    "In instance-based machine learning methods such as SVM or KNN, part of the data (or the entire data, in the worst case) constitute the resulting model. \n",
    "These methods are thus particularly exposed to reconstruction attacks (e.g. see [Yang et al. 2019](https://www.morganclaypool.com/doi/pdf/10.2200/S00960ED2V01Y201910AIM043)). \n",
    "In order to protect private information, we can apply Differential Privacy on the resulting model output from the clients and observe its influence on the federated global model. \n",
    "\n",
    "### Sensitivity by sampling:\n",
    "We first estimate model's sensitivity by sampling.\n",
    "Recall that the matrices of support vectors are the actual models' parameters, and that they can have differing number of rows.\n",
    "We then need to define a distance between such matrices: we can choose the max of the Euclidean distance of their rows (see [matrix distance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance_matrix.html)). \n",
    "\n",
    "\n",
    "Note that the `sk-learn` SVM solver is *non-deterministic*. In fact, due to the internal data shuffle, the SVM solver may deliver slightly different support vectors when training on the same set. Moreover, even when setting the random seed (see [`random_state` input parameter](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)), and simply switching one row in the training dataset, may result in slightly different output. \n",
    "As a matter of fact, in the sensitivity sampling, we consider databases that differ at most in one entry, or contain exactly the same data, yet some of the support vectors turn out to be different.\n",
    "This said, the sensitivity sampling procedure for this case is expected to deliver results with high variance.\n",
    "\n",
    "The resulting sensitivity is particularly high, and the application of DP dramatically deteriorates the performance of the global model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.differential_privacy import SensitivitySampler\n",
    "from shfl.differential_privacy import L1SensitivityNorm\n",
    "from shfl.differential_privacy import SensitivityNorm\n",
    "from scipy.spatial import distance_matrix\n",
    " \n",
    "class UniformDistribution(shfl.differential_privacy.ProbabilityDistribution):\n",
    "    \"\"\"\n",
    "    Implement Uniform sampling over the data\n",
    "    \"\"\"\n",
    "    def __init__(self, sample_data):\n",
    "        self._sample_data = sample_data\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        row_indices = np.random.choice(a=self._sample_data.shape[0], size=sample_size, replace=False)\n",
    "        \n",
    "        return self._sample_data[row_indices, :]\n",
    "    \n",
    "    \n",
    "class SVMClassifierSample(SVMClassifierModel):\n",
    "    \n",
    "    def get(self, data_array):\n",
    "        data = data_array[:, 0:-1]\n",
    "        labels = data_array[:, -1].astype(int)\n",
    "        params = np.array([], dtype=np.int64).reshape(0, (self._n_features + 1))\n",
    "        self.set_model_params(params)\n",
    "        train_model = self.train(data, labels)\n",
    "        model_params = self.get_model_params()\n",
    "        model_params = model_params[:,0:-1] # Exclude the classes indices\n",
    "        \n",
    "        return model_params.copy()\n",
    "\n",
    "\n",
    "class MatrixSetXoRNorm(SensitivityNorm):\n",
    "    \"\"\"\n",
    "    Distance matrix using only rows not in common.\n",
    "    \"\"\"\n",
    "    def compute(self, x_1, x_2):\n",
    "        nrows, ncols = x_1.shape\n",
    "        dtype = {'names':['f{}'.format(i) for i in range(ncols)],\n",
    "                   'formats':ncols * [x_1.dtype]}\n",
    "        x = np.setxor1d(x_1.view(dtype), x_2.view(dtype))\n",
    "        x = x.view(x_1.dtype).reshape(-1, ncols)\n",
    "        if x.shape[0] is not 0:\n",
    "            x = distance_matrix(x,x)\n",
    "            x = x.max()\n",
    "        else:\n",
    "            x = 0\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sampling database:\n",
    "n_instances = 400\n",
    "sampling_data, sampling_labels = make_classification(\n",
    "    n_samples=n_instances, n_features=n_features, n_informative=2, \n",
    "    n_redundant=0, n_repeated=0, n_classes=n_classes, \n",
    "    n_clusters_per_class=1, weights=None, flip_y=0.1, class_sep=0.1)  \n",
    "sample_data = np.hstack((sampling_data, sampling_labels.reshape(-1,1)))\n",
    "\n",
    "# Sampling sensitivity:\n",
    "distribution = UniformDistribution(sample_data)\n",
    "sampler = SensitivitySampler()\n",
    "\n",
    "n_samples = 200 # must be <= n_instances\n",
    "kwargs['random_state'] = 123\n",
    "max_sensitivity, mean_sensitivity = sampler.sample_sensitivity(\n",
    "    SVMClassifierSample(n_features=n_features, classes=classes, model=model_use), \n",
    "    MatrixSetXoRNorm(), distribution, n=n_samples, m=100)\n",
    "\n",
    "print(\"Max sensitivity from sampling: \" + str(max_sensitivity))\n",
    "print(\"Mean sensitivity from sampling: \" + str(mean_sensitivity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the federated learning experiment with differential privacy\n",
    "At this stage we are ready to add a layer of DP to our federated learning model. The Gaussian mechanism is employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.differential_privacy import GaussianMechanism\n",
    "\n",
    "sensitivity_array = np.full((n_features+1,), max_sensitivity)\n",
    "sensitivity_array[-1] = 0  # We don't apply noise on the classes\n",
    "params_access_definition = GaussianMechanism(sensitivity=sensitivity_array, epsilon_delta=(0.9, 0.9))\n",
    "federated_governmentDP = shfl.federated_government.FederatedGovernment(\n",
    "    model_builder, federated_data, aggregator, model_params_access=params_access_definition)\n",
    "\n",
    "federated_governmentDP.run_rounds(n=1, test_data=test_data, test_label=test_labels)\n",
    "if n_features == 2:\n",
    "    plot_2D_decision_boundary(federated_governmentDP.global_model, test_data, test_labels, title = \"Global model: Classification using Federated data\")\n",
    "\n",
    "print(\"Model used: \" + type(federated_governmentDP.global_model._model).__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity associated to the data: \n",
    "Since the SVM's parameters are constituted by the data itself, we might assume that the model's sensitivity is actually the sensitivity to apply on the data itself if one would try to access it (see [Laplace mechanism notebook](../differential_privacy/differential_privacy_laplace.ipynb)). \n",
    "We then take the component-wise variance of the data as the sensitivity. \n",
    "The resulting $\\epsilon$-private global model's performance is then comparable to the non-private version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.differential_privacy import GaussianMechanism\n",
    "\n",
    "sensitivity_array = np.var(sample_data, axis=0)\n",
    "sensitivity_array[-1] = 0  # We don't apply noise on the classes\n",
    "print(\"Component-wise sensitivity: \" + str(sensitivity_array))\n",
    "\n",
    "params_access_definition = GaussianMechanism(sensitivity=sensitivity_array, epsilon_delta=(0.9, 0.9))\n",
    "federated_governmentDP = shfl.federated_government.FederatedGovernment(\n",
    "    model_builder, federated_data, aggregator, model_params_access=params_access_definition)\n",
    "\n",
    "federated_governmentDP.run_rounds(n=1, test_data=test_data, test_label=test_labels)\n",
    "if n_features == 2:\n",
    "    plot_2D_decision_boundary(federated_governmentDP.global_model, test_data, test_labels, title = \"Global model: Classification using Federated data\")\n",
    "\n",
    "print(\"Model used: \" + type(federated_governmentDP.global_model._model).__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "\n",
    "**Remark 1: Federated learning round.** In this approach, the model's parameters are the actual support vectors. Thus, at each learning round, the support vectors are sent by the clients to the central server, where an additional SVM is run to aggregate the global model. At that stage, the (global) support vectors are sent back to the clients and are used *together* with clients' data to train the local model. However, the global support vectors are not considered as local data, and thus are not stored as client's data on the node.   <br>\n",
    "**Remark 2: Application of DP.** The model's sensitivity is highly responsive on the training data, and the resulting model's performance can be easily degenerated by application of DP. Sensitivity is computed both by sampling and data variance, and the former yields lower sensitivity. \n",
    "Nevertheless, neither of the two approaches fit in the definition of sensitivity based on either L1 and L2 norms, and a more general notion of distance should be introduced for a formal guarantee of DP (see [3.3 in Dwork et al. 2016](https://link.springer.com/chapter/10.1007/11681878_14)). <br>\n",
    "**Remark 3: Reduction of training data.** Since the SVM is particularly sensitive to duplicates in the training data, these are removed when fitting the model. However, when applying DP, there aren't essentially any identical instances and more sophisticated reduction techniques for training data should be used (e.g. a clustering technique as in [Yu et al. 2003](http://hanj.cs.illinois.edu/pdf/kdd03_scalesvm.pdf)) since otherwise the set of training vectors would *keep growing* at each federated round and introducing excessive noise in the model. When DP is applied as in this approach, it is thus advisable to run only a few federated rounds. <br>\n",
    "**Remark 4: Tuning for soft margin and kernel parameters.** In the presented case default values are used, however a tuning is in general needed for SVM models. Moreover,  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
