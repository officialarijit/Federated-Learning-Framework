{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated models: K-means clustering\n",
    "\n",
    "This notebook covers the problem of *unsupervised* learning in a federated configuration. \n",
    "In particular, a K-means clustering is used from the `sklearn` library (see [this link](https://scikit-learn.org/stable/modules/clustering.html#k-means)).\n",
    "This model is encapsulated in the Sherpa.ai FL platform and it is thus ready to use.  \n",
    "\n",
    "## The data\n",
    "The framework provides a function to load the [Iris](https://archive.ics.uci.edu/ml/datasets/iris) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import shfl\n",
    "import numpy as np\n",
    "from shfl.data_base.iris import Iris\n",
    "\n",
    "\n",
    "# Assign database:\n",
    "database = Iris()\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = database.load_data()\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "# Visualize training data: \n",
    "fig, ax = plt.subplots(1,2, figsize=(16,8))\n",
    "fig.suptitle(\"Iris database\", fontsize=20)\n",
    "ax[0].set_title('True labels', fontsize=18)\n",
    "ax[0].scatter(train_data[:, 0], train_data[:, 1], c=train_labels, s=150, edgecolor='k', cmap=\"plasma\")\n",
    "ax[0].set_xlabel('Sepal length', fontsize=18)\n",
    "ax[0].set_ylabel('Sepal width', fontsize=18)\n",
    "ax[0].tick_params(direction='in', length=10, width=5, colors='k', labelsize=20)\n",
    "\n",
    "ax[1].set_title('True labels', fontsize=18)\n",
    "ax[1].scatter(train_data[:, 2], train_data[:, 3], c=train_labels, s=150, edgecolor='k', cmap=\"plasma\")\n",
    "ax[1].set_xlabel('Petal length', fontsize=18)\n",
    "ax[1].set_ylabel('Petal width', fontsize=18)\n",
    "ax[1].tick_params(direction='in', length=10, width=5, colors='k', labelsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "We implement a method to plot K-means results in the Iris database and establish a centralized model, which will be our reference model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.model.kmeans_model import KMeansModel\n",
    "\n",
    "def plot_k_means(km, X, title):\n",
    "    new_labels = km.predict(X)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16,8))\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "    axes[0].scatter(X[:, 0], X[:, 1], c=new_labels, cmap='plasma', edgecolor='k', s=150)\n",
    "    axes[0].set_xlabel('Sepal length', fontsize=18)\n",
    "    axes[0].set_ylabel('Sepal width', fontsize=18)\n",
    "    axes[0].tick_params(direction='in', length=10, width=5, colors='k', labelsize=20)\n",
    "    axes[0].set_title('Predicted', fontsize=18)\n",
    "    \n",
    "    axes[1].scatter(X[:, 2], X[:, 3], c=new_labels, cmap='plasma', edgecolor='k', s=150)\n",
    "    axes[1].set_xlabel('Petal length', fontsize=18)\n",
    "    axes[1].set_ylabel('Petal width', fontsize=18)\n",
    "    axes[1].tick_params(direction='in', length=10, width=5, colors='k', labelsize=20) \n",
    "    axes[1].set_title('Predicted', fontsize=18)\n",
    "    \n",
    "# Plot training data:\n",
    "centralized_model = KMeansModel(n_clusters=3, n_features = train_data.shape[1])\n",
    "centralized_model.train(train_data)\n",
    "\n",
    "print(centralized_model.get_model_params())\n",
    "plot_k_means(centralized_model, train_data, title = \"Benchmark: K-means using centralized data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to aggregate a model's parameters from each federated node in clustering\n",
    "\n",
    "Since the labels of clusters can vary among each node, we cannot average the centroids right away. \n",
    "One solution is to choose the lowest distance average: this is achieved by simply applying the K-means algorithm to the centroids coordinates of all nodes. \n",
    "In [ClusterFedAvgAggregator](https://github.com/sherpaai/Sherpa.ai-Federated-Learning-Framework/blob/master/shfl/federated_aggregator/cluster_fedavg_aggregator.py), you can see its implementation.\n",
    "\n",
    "**Note**: This implementation is based on the assumption that the number of clusters has been previously fixed across the clients, so it only works properly in IID scenarios (see [Federated Sampling](../federated_learning/federated_learning_sampling.ipynb)). We are working in a federated aggregation operator, which works in every distribution of data, across clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.federated_aggregator.cluster_fedavg_aggregator import ClusterFedAvgAggregator\n",
    "\n",
    "# Create the IID data: \n",
    "iid_distribution = shfl.data_distribution.IidDataDistribution(database)\n",
    "federated_data, test_data, test_label = iid_distribution.get_federated_data(num_nodes = 12, percent=100)\n",
    "print(\"Number of nodes: \" + str(federated_data.num_nodes()))\n",
    "\n",
    "# Run the algorithm:\n",
    "aggregator = ClusterFedAvgAggregator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the federated learning experiment\n",
    "\n",
    "We are now ready to run our model in a federated configuration. \n",
    "\n",
    "The performance is assessed by several clustering metrics (see [this link](https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation)).\n",
    "\n",
    "For reference, below, we compare the metrics of:\n",
    " - Each node \n",
    " - The global (federated) model\n",
    " - The centralized (non-federated) model\n",
    " \n",
    "It can be observed that the performance of the global federated model is superior, in general, with respect to the performance of each node. Thus, the federated learning approach proves to be beneficial. Moreover, the performance of the global federated model is very close to the performance of the centralized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.federated_government.federated_government import FederatedGovernment\n",
    "\n",
    "n_clusters = 3 # Set number of clusters\n",
    "n_features = train_data.shape[1]\n",
    "def model_builder():\n",
    "    model = KMeansModel(n_clusters=n_clusters, n_features = n_features)\n",
    "    return model\n",
    "\n",
    "\n",
    "federated_government = FederatedGovernment(model_builder, federated_data, aggregator)\n",
    "print(\"Test data size: \" + str(test_data.shape[0]))\n",
    "print(\"\\n\")\n",
    "federated_government.run_rounds(n = 3, test_data = test_data, test_label = test_label)\n",
    "\n",
    "# Reference centralized (non federate) model:\n",
    "print(\"Centralized model test performance : \" + str(centralized_model.evaluate(data=test_data, labels=test_labels)))\n",
    "plot_k_means(centralized_model, test_data, title = \"Benchmark on Test data: K-means using CENTRALIZED data\")\n",
    "plot_k_means(federated_government.global_model, test_data, title = \"Benchmark on Test data: K-means using FL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add differential privacy\n",
    "\n",
    "To preserve client privacy, in this section, we are going to introduce Differential Privacy (DP) into our model. First, we calibrate the noise introduced by the differentially private mechanism using the training data, then we apply DP to each client feature, so that each cluster computed by a client is shared with the main server privately; that is, without disclosing the identity of the client.\n",
    "\n",
    "### Model's sensitivity\n",
    "In the case of applying the Gaussian privacy mechanism, the noise added has to be of the same order as the sensitivity of the model's output, i.e., the coordinates of each cluster.\n",
    "\n",
    "In the general case, the model's sensitivity might be difficult to compute analytically. \n",
    "An alternative approach is to attain random differential privacy through a sampling over the data.\n",
    "\n",
    "That is, instead of computing the global sensitivity $\\Delta f$ analytically, we compute an empirical estimation of it, by sampling over the dataset.\n",
    "This approach is very convenient, since it allows for the sensitivity estimation of an arbitrary model or a black-box computer function.\n",
    "The Sherpa.ai Federated Learning and Differential Privacy Framework provides this functionality in the class `SensitivitySampler`.\n",
    "\n",
    "In order to carry out this approach, we need to specify a distribution of the data to sample from. \n",
    "Generally, this requires previous knowledge and/or model assumptions. \n",
    "However, in our specific case of manufactured data, we can assume that the data distribution is uniform. \n",
    "We define our class of `ProbabilityDistribution` that uniformly samples over a data-frame.\n",
    "Moreover, we assume that we have access to a set of data (this can be thought of, for example, as a public data set). \n",
    "In this example, we generate a new dataset, and use its training partition for sampling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class UniformDistribution(shfl.differential_privacy.ProbabilityDistribution):\n",
    "    \"\"\"\n",
    "    Implement Uniform sampling over the data\n",
    "    \"\"\"\n",
    "    def __init__(self, sample_data):\n",
    "        self._sample_data = sample_data\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        row_indices = np.random.randint(low=0, high=self._sample_data.shape[0], size=sample_size, dtype='l')\n",
    "        \n",
    "        return self._sample_data[row_indices, :]\n",
    "    \n",
    "sample_data = train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `SensitivitySampler` implements the sampling, given a query, i.e., the learning model itself, in this case.\n",
    "We only need to add the `get` method to our model since it is required by the class `SensitivitySampler`. \n",
    "We choose the sensitivity norm to be the v norm and we apply the sampling. \n",
    "The value of the sensitivity depends on the number of samples `n`: the more samples we perform, the more accurate the sensitivity. \n",
    "Indeed, upon increasing the number of samples `n`, the sensitivity becomes more accurate and typically decreases. \n",
    "\n",
    "Unfortunately, sampling over a dataset involves the training of the model on two datasets differing in one entry, at each sample.\n",
    "Thus, in general, this procedure might be computationally expensive (e.g., in the case of training a deep neural network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.differential_privacy import SensitivitySampler\n",
    "from shfl.differential_privacy import L2SensitivityNorm\n",
    "\n",
    "class KMeansSample(KMeansModel):\n",
    "    \n",
    "    def __init__(self, feature, **kargs):\n",
    "        self._feature = feature\n",
    "        super().__init__(**kargs)\n",
    "    \n",
    "    def get(self, data_array):\n",
    "        self.train(data_array)\n",
    "        params = self.get_model_params()\n",
    "        return params[:, self._feature]\n",
    "\n",
    "distribution = UniformDistribution(sample_data)\n",
    "sampler = SensitivitySampler()\n",
    "# Reproducibility\n",
    "np.random.seed(789)\n",
    "n_samples = 50\n",
    "\n",
    "sensitivities = np.empty(n_features)\n",
    "\n",
    "for i in range(n_features):\n",
    "    model = KMeansSample(feature=i, n_clusters=n_clusters, n_features=n_features)\n",
    "    sensitivities[i], _ = sampler.sample_sensitivity(model, L2SensitivityNorm(), distribution, n=n_samples, gamma=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max sensitivity from sampling: \", np.max(sensitivities))\n",
    "print(\"Min sensitivity from sampling: \", np.min(sensitivities))\n",
    "print(\"Mean sensitivity from sampling:\", np.mean(sensitivities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, if the model has more than one feature, it is a bad idea to estimate the sensitivity for all of the features at the same time, as the features may have wildly varying sensitivities. In this case, we estimate the sensitivity for each feature. Note that we provide the array of estimated sensitivities to the GaussianMechanism and apply it to each feature individually.\n",
    "\n",
    "### Run the federated learning experiment with differential privacy\n",
    "\n",
    "At this stage we are ready to add the layer of DP to our federated learning model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.differential_privacy import GaussianMechanism\n",
    "\n",
    "dpm = GaussianMechanism(sensitivity=sensitivities, epsilon_delta=(0.9, 0.9))\n",
    "federated_government = FederatedGovernment(\n",
    "    model_builder, federated_data, aggregator, model_params_access = dpm)\n",
    "print(\"Test data size: \" + str(test_data.shape[0]))\n",
    "print(\"\\n\")\n",
    "federated_government.run_rounds(n = 1, test_data = test_data, test_label = test_label)\n",
    "\n",
    "# Reference Centralized (non federate) model:\n",
    "print(\"Centralized model test performance : \" + str(centralized_model.evaluate(data=test_data, labels=test_labels)))\n",
    "plot_k_means(centralized_model, test_data, title = \"Benchmark on Test data: K-means using CENTRALIZED data\")\n",
    "plot_k_means(federated_government.global_model, test_data, title = \"Benchmark on Test data: K-means using FL and DP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, when we add DP to the model, it becomes quite unstable (multiple executions each one with very different results) and almost useless (even with unacceptable values for $\\delta$, that is $\\delta \\geq 0.5$, the results are quite bad), which suggests that another way of adding DP has to be provided. An alternative approach for adding DP can be found in [A differential privacy protecting K-means clustering algorithm based on contour coefficients](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0206832), but it is still unclear as to how to adapt it to a federated setting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SherpaFL_py37",
   "language": "python",
   "name": "sherpafl_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
