{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential privacy: composition concepts\n",
    "\n",
    "This notebook is a continuation of the notebook [Basic Concepts](./differential_privacy_basic_concepts.ipynb). Here, we explain more advanced Differential Privacy (DP) concepts, such as composition theorems and how to use them in the Sherpa.ai Federated Learning and Differential Privacy Framework. Before diving in, we recommend reading section 3.5 of [The Algorithmic Foundations of Differential Privacy](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf) and everything related to Privacy Filters from the paper [Privacy Odometers and Filters: Pay-as-you-Go Composition](https://arxiv.org/pdf/1605.08294.pdf).\n",
    "\n",
    "## Composition theorems\n",
    "\n",
    "A great property of DP is that private mechanisms can be composed while preserving DP. The new values of $\\epsilon$ and $\\delta$ can be computed according to the composition theorems. Before the composition theorems are provided, we are going to state an experiment with an adversarial, which proposes a composition scenario for DP.\n",
    "\n",
    "**Composition experiment $b \\in \\{ 0,1 \\}$ for adversary $A$ with a given set, $M$, of DP mechanisms:**\n",
    "\n",
    "For $i=1,\\dots,k$:\n",
    "\n",
    "1. $A$ generates two neighbouring databases $x_i^0$ and $x_i^1$ and selects a mechanism $\\mathcal{M}_i$ from $M$.\n",
    "2. $A$ receives the output $y_i \\in \\mathcal{M}_i(x_i^b)$, which is stored in $V^b$\n",
    "\n",
    "Note that the adversary is stateful, that is, it stores the output in each iteration and selects the DP mechanism  based on the observed outputs.\n",
    "\n",
    "**Note on neighboring databases:**\n",
    "\n",
    "It is important to know that when it comes to numeric databases, such as two arrays, $A=[1,2,3,4]$ and $B=[1,2,3,8]$ (which is the main use case for the Sherpa.ai Federated Learning and Differential Privacy Framework). They are neighboring databases if they differ in only one component, up to as much as 1 (the must have the same length), therefore A and B aren't neighboring databases but, $C=[1,28,91]$ and $D=[2,28,91]$ are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.private.node import DataNode\n",
    "from shfl.differential_privacy.dp_mechanism import LaplaceMechanism, GaussianMechanism\n",
    "from math import log, exp\n",
    "import numpy as np\n",
    "\n",
    "def run_composition_experiment(M, db_storage, secret):\n",
    "    # Number of runs equals the number of mechanisms provided\n",
    "    k = len(M)\n",
    "    \n",
    "    # Adversary's view in experiment 1\n",
    "    A_view1 = np.empty(shape=(k,))\n",
    "    # Adversary's view in experiment 2\n",
    "    A_view2 = np.empty(shape=(k,))\n",
    "\n",
    "    # Neighboring databases are created\n",
    "    db1 = \"db1\"\n",
    "    db2 = \"db2\"\n",
    "    db_storage.set_private_data(name=db1, data=secret)\n",
    "    db_storage.set_private_data(name=db2, data=secret+1)\n",
    "\n",
    "    # In the following loop, we reproduce both experiments for b=0 and for b=1\n",
    "    for i in range(k):\n",
    "        # The adversarial selects the dp-mechanism\n",
    "        db_storage.configure_data_access(db1, M[i])\n",
    "        db_storage.configure_data_access(db2, M[i])\n",
    "        # The outputs are stored in the adversary's view in each experiment\n",
    "        A_view1[i] = db_storage.query(db1)\n",
    "        A_view2[i] = db_storage.query(db2)\n",
    "    \n",
    "    return A_view1, A_view2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the following piece of code, privacy is preserved, as it is not possible to tell in which database the secret is stored. However, if this experiment is run enough times, the probability of telling the difference increases, so what is the privacy budget spent in these experiments? This is the fundamental question that composition theorems answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup storage for all databases\n",
    "db_storage = DataNode()\n",
    "\n",
    "# List of DP-mechanisms\n",
    "M = [LaplaceMechanism(1, epsilon=0.5), \n",
    "     LaplaceMechanism(1, epsilon=1), \n",
    "     GaussianMechanism(1, epsilon_delta=(0.5, 0.01))]\n",
    "\n",
    "A_view1, A_view2 = run_composition_experiment(M, db_storage, 1)\n",
    "    \n",
    "print(\"Adversary's view from Experiment 1: {}, mean: {}\".format(A_view1, np.mean(A_view1)))\n",
    "print(\"Adversary's view from Experiment 2: {}, mean: {}\".format(A_view2, np.mean(A_view2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, if the experiment is carried on for enough rounds, we can determine in which database the secret is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup storage for all databases\n",
    "db_storage = DataNode()\n",
    "\n",
    "# List of DP-mechanisms\n",
    "M = [LaplaceMechanism(1, epsilon=0.5), \n",
    "     LaplaceMechanism(1, epsilon=1), \n",
    "     GaussianMechanism(1, epsilon_delta=(0.5, 0.01))]*1000\n",
    "\n",
    "A_view1, A_view2 = run_composition_experiment(M, db_storage, 1)\n",
    "    \n",
    "print(\"Adversary's view from Experiment 1 mean: {}\".format(np.mean(A_view1)))\n",
    "print(\"Adversary's view from Experiment 2 mean: {}\".format(np.mean(A_view2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic composition theorem\n",
    "The first and most basic theorem that can be employed for composition is the Basic Composition Theorem.\n",
    "The composition of a sequence $\\{\\mathcal{M}_k\\}$ of ($\\epsilon_i, \\delta_i$)-differentially private mechanisms under the Composition experiment with $M=\\{\\mathcal{M}_k\\}$, is ($\\sum_{i=1}^{k} \\epsilon_i, \\sum_{i=1}^{k} \\delta_i$)-differentially private.\n",
    "\n",
    "In other words, it states that the resulting privacy budget is the sum of the privacy budget spent in each access. \n",
    "Therefore, the budget expended in the previous experiment was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_delta_access = [m.epsilon_delta for m in M]\n",
    "epsilon_spent, delta_spent = map(sum, zip(*epsilon_delta_access))\n",
    "print(\"{} epsilon was spent\".format(epsilon_spent))\n",
    "print(\"{} delta was spent\".format(delta_spent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main disadvantage of this theorem is that it assumes a worst case scenario. A better bound can be stated using the Advanced Composition Theorem.\n",
    "\n",
    "### Advanced composition theorem\n",
    "\n",
    "For all $\\epsilon, \\delta, \\delta' \\geq 0$ the composition of a sequence $\\{\\mathcal{M}_k\\}$ of ($\\epsilon, \\delta$)-differentially private mechanisms under the Composition experiment with $M=\\{\\mathcal{M}_k\\}$, satisfies ($\\epsilon', \\delta''$)-DP with:\n",
    "\n",
    "$$\n",
    "\\epsilon' = \\sqrt{2k\\ln(1/\\delta')} + k \\epsilon(e^{\\epsilon}-1) \\quad \\text{and} \\quad \\delta'' = k\\delta + \\delta'\n",
    "$$\n",
    "\n",
    "In other words, for a small sacrifice $\\delta$' in the global $\\delta$ spent, we can achieve a better bound for the global $\\epsilon$ spent. However, the theorem assumes that the same DP mechanism is used in each access:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt, log, exp\n",
    "\n",
    "# Basic theorem computations\n",
    "def basic_theorem_expense(epsilon, delta, k):\n",
    "    epsilon_spent = k*epsilon\n",
    "    delta_spent = k*delta\n",
    "    return epsilon_spent, delta_spent\n",
    "\n",
    "# Advanced theorem computations\n",
    "def advanced_theorem_expense(epsilon, delta, delta_sacrifice, k):\n",
    "    epsilon_spent = sqrt(2*k*log(1/delta_sacrifice)) + k * epsilon * (exp(epsilon) - 1)\n",
    "    delta_spent = k*delta + delta_sacrifice\n",
    "    return epsilon_spent, delta_spent\n",
    "\n",
    "\n",
    "epsilon = 0.5\n",
    "delta = 0\n",
    "k = 3\n",
    "delta_sacrifice = 0.1\n",
    "\n",
    "basic = basic_theorem_expense(epsilon, delta, k)\n",
    "advanced = advanced_theorem_expense(epsilon, delta, delta_sacrifice, k)\n",
    "\n",
    "print(\"Epsilon: {} vs {} (basic theorem vs advanced theorem) \".format(basic[0], advanced[0]))\n",
    "print(\"Delta: {} vs {} (basic theorem vs advanced theorem) \".format(basic[1], advanced[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait, if the epsilon spent is worse with the new theorem, is it useless? Of course not, let's see what happens when we increase the number of iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt, log, exp\n",
    "\n",
    "epsilon = 0.5\n",
    "delta = 0\n",
    "k = 350\n",
    "delta_sacrifice = 0.1\n",
    "\n",
    "basic = basic_theorem_expense(epsilon, delta, k)\n",
    "advanced = advanced_theorem_expense(epsilon, delta, delta_sacrifice, k)\n",
    "\n",
    "print(\"Epsilon: {} vs {} (basic theorem vs advanced theorem) \".format(basic[0], advanced[0]))\n",
    "print(\"Delta: {} vs {} (basic theorem vs advanced theorem) \".format(basic[1], advanced[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can conclude that the benefits of the advanced theorem are only noticeable when the number of mechanism accesses is huge. In particular, we can observe that for values of $k$ close to 150 (and $\\delta=0.1$), the theorems are almost identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16,6))\n",
    "k_values = np.arange(1, 300, 5)\n",
    "ax.plot(k_values, [basic_theorem_expense(epsilon, delta, k)[0] for k in k_values], label = \"Basic composition\")  \n",
    "ax.plot(k_values, [advanced_theorem_expense(epsilon, delta, delta_sacrifice, k)[0] for k in k_values], label = \"Advanced composition\")   \n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('$\\epsilon$ expense')\n",
    "\n",
    "plt.legend(title = \"\", loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While composition theorems are quite useful, they require some parameters to be defined upfront, such as the number of mechanisms to be composed. Therefore, no intermediate result can be observed and the privacy budget can be wasted. In such situations, a more fine grained composition technique, which allows to observe the result of each mechanism to be observed without compromising the privacy budget spent, is required. In order to remove some of the stated constraints, a more flexible experiment of composition can be introduced:\n",
    "\n",
    "### Adaptive composition experiment $b \\in \\{ 0,1 \\}$ for adversary $A$\n",
    "\n",
    "For $i=1,\\dots,k$:\n",
    "\n",
    "1. $A$ generates two neighboring databases $x_i^0$ and $x_i^1$ and selects a mechanism $\\mathcal{M}_i$ that is ($\\epsilon_i, \\delta_i$)-differentially private.\n",
    "2. $A$ receives the output $y_i \\in \\mathcal{M}_i(x_i^b)$\n",
    "\n",
    "Note that in these situations, the $\\epsilon_i$ and $\\delta_i$ of each mechanism is adaptively selected, based on the outputs of previous iterations.\n",
    "\n",
    "Now we introduce the privacy filter, which can be used to guarantee with high probability in the Adaptive Composition experiments, the stated privacy budget $\\epsilon_g$ is never exceeded. Privacy filters have similar composition theorems to those mentioned previously:\n",
    "\n",
    "### Basic composition for privacy filters\n",
    "\n",
    "For any $\\epsilon_g, \\delta_g \\geq 0,\\ $ $\\texttt{COMP}_{\\epsilon_g, \\delta_g}$ is a valid Privacy Filter: \n",
    "\n",
    "$$\n",
    " \\texttt{COMP}_{\\epsilon_g,\\delta_g}(\\epsilon_1,\\delta_1,...,\\epsilon_{k},\\delta_{k})= \\begin{cases} \n",
    "      \\texttt{HALT} & \\text{if}\\  \\sum_{i=1}^{k} \\delta_i > \\delta_g \\ \\ \\ \\text{or} \\ \\ \\ \\sum_{i=1}^{k} \\epsilon_i > \\epsilon_g, \\\\\n",
    "      \\texttt{CONT} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Advanced composition for privacy filters\n",
    "\n",
    "We define $\\mathcal{K}$ as follows:\n",
    "$$\n",
    "    \\mathcal{K} :=  \\sum_{j=1}^{k} \\epsilon_j \\left( \\frac{\\exp{(\\epsilon_j)}-1}{2} \\right) + \n",
    "                \\sqrt{\\left( \\sum_{i=1}^{k} \\epsilon_i^2 + H \\right) \\left( 2 + \\ln{\\big( \\frac{1}{H} \\sum_{i=1}^{k} \\epsilon_i^2 +1 \\big)} \\right) \\ln{(2/\\delta_g)}}\n",
    "$$\n",
    "\n",
    "with $$ H = \\frac{\\epsilon_g^2}{28.04 \\ln(1/\\delta_g)} $$\n",
    " \n",
    "Then $\\texttt{COMP}_{\\epsilon_g, \\delta_g}$ is a valid Privacy Filter for $\\delta_g \\in (0, 1/e)$ and $\\epsilon_g > 0$, where:\n",
    "$$\n",
    " \\texttt{COMP}_{\\epsilon_g,\\delta_g}(\\epsilon_1,\\delta_1,...,\\epsilon_{k},\\delta_{k})= \\begin{cases} \n",
    "      \\texttt{HALT} & \\text{if}\\  \\sum_{i=1}^{k} \\delta_i > \\delta_g/2 \\ \\ \\ \\text{or} \\ \\ \\ \\mathcal{K} > \\epsilon_g, \\\\\n",
    "      \\texttt{CONT} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The value of $\\mathcal{K}$ might be strange at first sight, however, if we assume $\\epsilon_j=\\epsilon$ for all $j$, it remains:\n",
    "$$\n",
    "    \\mathcal{K} = \\sqrt{ \\left(k\\epsilon^2 + H\\right)\\left(2+\\ln{(\\frac{k\\epsilon^2}{H} + 1)}\\right) \\ln{(2/\\delta)}} + k\\epsilon^2 \\left(\\frac{\\exp{(\\epsilon)}-1}{2}\\right)\n",
    "$$\n",
    "which is quite similar to the expression given in the Advanced Composition Theorem.\n",
    "\n",
    "\n",
    "\n",
    "## Privacy filters in the Sherpa.ai Federated Learning and Differential Privacy Framework\n",
    "\n",
    "This framework implements Privacy Filters and transparently applies both theorems stated before, so that there is no need to constantly check which theorem ensures a better ($\\epsilon, \\delta$) expense. When the fixed privacy budget is surpassed, an exception ExceededPrivacyBudgetError is raised. The following example shows two equivalent implementations of the Adaptive Composition experiment, stated before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.private.node import DataNode\n",
    "from shfl.differential_privacy.composition_dp import AdaptiveDifferentialPrivacy\n",
    "from shfl.differential_privacy.composition_dp import ExceededPrivacyBudgetError\n",
    "from shfl.differential_privacy.dp_mechanism import LaplaceMechanism\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def run_adaptive_comp_experiment_v1(global_eps_delta, eps_delta_access):\n",
    "    # Define a place to store the data\n",
    "    node_single = DataNode()\n",
    "\n",
    "    # Store the private data\n",
    "    node_single.set_private_data(name=\"secret\", data=np.array([1]))\n",
    "\n",
    "    # Choose your favorite differentially_private_mechanism\n",
    "    dpm = LaplaceMechanism(sensitivity=1, epsilon=eps_delta_access)\n",
    "\n",
    "    # Here we are specifying that we want to use composition theorems for Privacy Filters\n",
    "    # DP mechanis\n",
    "    default_data_access = AdaptiveDifferentialPrivacy(global_eps_delta, differentially_private_mechanism=dpm)\n",
    "    node_single.configure_data_access(\"secret\", default_data_access)\n",
    "\n",
    "    result_query = []\n",
    "    while True:\n",
    "        try:\n",
    "            # Queries are performed using the Laplace mechanism\n",
    "            result_query.append(node_single.query(private_property=\"secret\"))\n",
    "        except ExceededPrivacyBudgetError:\n",
    "            # At this point we have spent the entiry privacy budget\n",
    "            break       \n",
    "    \n",
    "    return result_query\n",
    "\n",
    "def run_adaptive_comp_experiment_v2(global_eps_delta, eps_delta_access):\n",
    "    # Define a place to store the data\n",
    "    node_single = DataNode()\n",
    "\n",
    "    # Store the private data\n",
    "    node_single.set_private_data(name=\"secret\", data=np.array([1]))\n",
    "\n",
    "    # Choose your favorite differentially_private_mechanism\n",
    "    dpm = LaplaceMechanism(sensitivity=1, epsilon=eps_delta_access)\n",
    "\n",
    "    # Here we are specifying that we want to use composition theorems for Privacy Filters\n",
    "    default_data_access = AdaptiveDifferentialPrivacy(global_eps_delta)\n",
    "    node_single.configure_data_access(\"secret\", default_data_access)\n",
    "\n",
    "    result_query = []\n",
    "    while True:\n",
    "        try:\n",
    "            # DP mechanism is specified at time of query, in this case the Laplace mechanism\n",
    "            # if no mechanism is specified an exception is raised\n",
    "            result_query.append(node_single.query(private_property=\"secret\", differentially_private_mechanism=dpm))\n",
    "        except ExceededPrivacyBudgetError:\n",
    "            # At this point we have spent the entire privacy budget\n",
    "            break            \n",
    "            \n",
    "    return result_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot, we can see that the privacy budget is spent significantly faster, as $\\epsilon$ moves away from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_epsilon_delta = (2e-1, 2e-30)  \n",
    "epsilon_values = np.arange(2e-3, 2e-1, 2e-3)\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16,6))\n",
    "y_axis=[len(run_adaptive_comp_experiment_v1(global_epsilon_delta, e)) for e in epsilon_values] \n",
    "ax.plot(epsilon_values, y_axis)  \n",
    "ax.set_xlabel('$\\epsilon$')\n",
    "ax.set_ylabel('Number of runs before the budget is spent')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "These experiments are run with the same DP mechanism, for the sake of simplification. If you want to access your data with a different DP mechanism, we recommend using a schema similar to the one shown in the function *run_adaptive_comp_experiment_v2*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
