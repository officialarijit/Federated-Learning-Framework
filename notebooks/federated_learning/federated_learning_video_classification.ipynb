{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated learning: video classification\n",
    "The present experiment tackles the **federated video classification** using Sherpa Federated Learning (FL) Framework with Keras ([this tutorial](https://www.pyimagesearch.com/2019/07/15/video-classification-with-keras-and-deep-learning/) has been used as reference).\n",
    "The main contribution of this study case is double. \n",
    "First of all, we assess the performance of a video classification problem in the federated context. \n",
    "Moreover, we demonstrate how easily we can set up a federated simulation using an existing (centralized) machine learning instance.\n",
    "\n",
    "## The data\n",
    "If not done already, we first need to download the dataset. \n",
    "It is a collection of labeled images of 12 different sports. \n",
    "For simplicity and for saving computational resources, we decide to only train for three sports, e.g. `tennis`, `football` and `weight_lifting`. However, more categories can be employed by modifying the `LABELS` object below. \n",
    "We also set the input parameters such as paths where to save and load the objects, and the number of federated learning rounds to run. Moreover, we can set the boolean parameter `train_network` to either train the federated neural network, or use a pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "import shfl\n",
    "\n",
    "# Set the input parameters for the experiment:\n",
    "args = {\"data_path\":\"../data/video_classification/sports\", \n",
    "        \"output_path\": \"../data/video_classification/output\",\n",
    "        \"input_path\": \"../data/video_classification/example_clips\",\n",
    "        \"model_name\":\"activity.model\", \n",
    "        \"label_bin\": \"lb.pickle\", \n",
    "        \"federated_rounds\":3,\n",
    "        \"size_averaging\": 1, \n",
    "        \"train_network\": True}\n",
    "\n",
    "LABELS = set([\"weight_lifting\", \"tennis\", \"football\"])\n",
    "print(\"[INFO] training for labels: \" + str(LABELS))\n",
    "\n",
    "if not os.path.exists(args[\"data_path\"]):\n",
    "    print(\"[INFO] creating data folders and cloning dataset repo ...\")\n",
    "    subprocess.run([\"git\", \"clone\",  \"https://github.com/jurjsorinliviu/Sports-Type-Classifier\", args[\"data_path\"]])\n",
    "if not os.path.exists(args[\"output_path\"]):\n",
    "    os.mkdir(args[\"output_path\"])\n",
    "    \n",
    "    \n",
    "print(\"[INFO] loading images...\")\n",
    "imagePaths = list(paths.list_images(os.path.join(args[\"data_path\"], \"data/\")))\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# loop over the image paths\n",
    "for imagePath in imagePaths:\n",
    "    # extract the class label from the filename\n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "    # if the label of the current image is not part of of the labels\n",
    "    # are interested in, then ignore the image\n",
    "    if label not in LABELS:\n",
    "        continue\n",
    "\n",
    "    # load the image, convert it to RGB channel ordering, and resize\n",
    "    # it to be a fixed 224x224 pixels, ignoring aspect ratio\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "\n",
    "    # update the data and labels lists, respectively\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "\n",
    "# convert the data and labels to NumPy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# perform one-hot encoding on the labels\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is created, it is encapsulated in a Sherpa.FL dataset, and the train set is easily independently and identically distributed (IID) over the network of nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a labeled database for shfl:\n",
    "database = shfl.data_base.LabeledDatabase(data, labels)\n",
    "train_data, train_labels, test_data, test_labels = database.load_data()\n",
    "\n",
    "print(\"[INFO] Number of train images: \" + str(len(train_data)))\n",
    "print(\"[INFO] Number of test images: \" + str(len(test_data)))\n",
    "\n",
    "# Distribute data over the federated network of nodes:\n",
    "print(\"[INFO] Distributing the train set across the nodes...\")\n",
    "iid_distribution = shfl.data_distribution.IidDataDistribution(database)\n",
    "federated_data, test_data, test_label = iid_distribution.get_federated_data(num_nodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "At this point, we can create our Keras model. \n",
    "We start from a pre-trained network, and we will only train the last layers (fine-tuning).\n",
    "An augmentation of the train set will be employed. \n",
    "We define the learning model class and a model builder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# initialize the training data augmentation object\n",
    "trainAug = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\")\n",
    "\n",
    "# initialize the validation/testing data augmentation object (which\n",
    "# we'll be adding mean subtraction to)\n",
    "valAug = ImageDataGenerator()\n",
    "\n",
    "# define the ImageNet mean subtraction (in RGB order) and set the\n",
    "# the mean subtraction value for each of the data augmentation\n",
    "# objects\n",
    "mean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\n",
    "trainAug.mean = mean\n",
    "valAug.mean = mean\n",
    "\n",
    "\n",
    "class DeepLearningModelAug(shfl.model.DeepLearningModel):\n",
    "    def train(self, data, labels):\n",
    "        self._check_data(data)\n",
    "        self._check_labels(labels)\n",
    "\n",
    "        #early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='min')\n",
    "        self._model.fit(x=trainAug.flow(data, labels, batch_size=32),\n",
    "            steps_per_epoch=len(data) // 32,\n",
    "            validation_data=valAug.flow(test_data, test_labels),\n",
    "            validation_steps=len(test_data) // 32,\n",
    "            epochs=self._epochs)\n",
    "\n",
    "def model_builder():\n",
    "    \n",
    "    # load the ResNet-50 network, where head FC layer sets are left off\n",
    "    baseModel = ResNet50(weights=\"imagenet\", include_top=False,\n",
    "        input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "    # construct the head of the model that will be placed on top of the\n",
    "    # the base model\n",
    "    headModel = baseModel.output\n",
    "    headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "    headModel = Flatten(name=\"flatten\")(headModel)\n",
    "    headModel = Dense(512, activation=\"relu\")(headModel)\n",
    "    headModel = Dropout(0.5)(headModel)\n",
    "    headModel = Dense(len(lb.classes_), activation=\"softmax\")(headModel)\n",
    "\n",
    "    # place the head FC model on top of the base model (this will become\n",
    "    # the actual model we will train)\n",
    "    model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "    # freeze pre-trained base model layers so they will\n",
    "    # *not* be updated during the training process\n",
    "    for layer in baseModel.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    opt = SGD(lr=1e-4, momentum=0.9, decay=1e-4 / args[\"federated_rounds\"])\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "    return DeepLearningModelAug(model, epochs=epochs_per_FL_round)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the federated learning experiment\n",
    "Now we're ready to run the federated learning experiment. \n",
    "We choose the global model to be the average of the locally trained models by setting the aggregator to `FedAvgAggregator()`, we create the federated government and we run the FL rounds. \n",
    "Each client will train a local model using its own private data. Only a single epoch is performed (`epochs_per_FL_round=1`), then the local models are aggregated to form the global model, thus completing an FL round (note that this step is computationally demanding and could require several minutes depending on the machine used):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network:\n",
    "epochs_per_FL_round=1\n",
    "aggregator = shfl.federated_aggregator.FedAvgAggregator()\n",
    "federated_government = shfl.federated_government.FederatedGovernment(model_builder, federated_data, aggregator)\n",
    "if args[\"train_network\"]:\n",
    "    federated_government.run_rounds(args[\"federated_rounds\"], test_data, test_label)\n",
    "else:\n",
    "    print(\"[INFO] loading pre-computed model ...\")\n",
    "    model_path = os.path.join(args[\"output_path\"], args[\"model_name\"])\n",
    "    federated_government.global_model._model = load_model(model_path)\n",
    "    lb = pickle.loads(open(os.path.join(args[\"output_path\"], args[\"label_bin\"]), \"rb\").read())\n",
    "    \n",
    "# evaluate the federated network\n",
    "print(\"[INFO] evaluating federated network...\")\n",
    "predictions = federated_government.global_model.predict(data=test_data.astype(\"float32\"))\n",
    "print(classification_report(test_labels.argmax(axis=1),\n",
    "    predictions, target_names=lb.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the performance of the trained network using the test set. \n",
    "The federated model is compared with the centralized model, where, for a fair comparison, the latter is trained with the same number of epochs as the FL rounds (the training of the centralized model is faster compared to the federated case, since only one single model is used). \n",
    "It can be observed that the two models exhibit a similar performance, allowing us to conclude that the FL approach can produce a high accuracy model while at the same time preserving data privacy of the clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison with Centralized experiment: \n",
    "if args[\"train_network\"]:\n",
    "    epochs_per_FL_round=args[\"federated_rounds\"]\n",
    "    centralized_model = model_builder()\n",
    "    centralized_model.train(train_data, train_labels)\n",
    "\n",
    "    # evaluate the centralized network\n",
    "    print(\"[INFO] evaluating centralized network...\")\n",
    "    predictions_centralized = centralized_model.predict(data=test_data.astype(\"float32\"))\n",
    "    print(classification_report(test_labels.argmax(axis=1),\n",
    "        predictions_centralized, target_names=lb.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training of the federated model can be time-consuming since, although using less data, each client needs to train a local model. \n",
    "We can serialize the trained model to disk for later use in the classification of videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the model to disk\n",
    "if args[\"train_network\"]:\n",
    "    model_path = os.path.join(args[\"output_path\"], args[\"model_name\"])\n",
    "    federated_government.global_model._model.save(model_path, save_format=\"h5\")\n",
    "    f = open(os.path.join(args[\"output_path\"], args[\"label_bin\"]), \"wb\")\n",
    "    f.write(pickle.dumps(lb))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction: video classification\n",
    "We can test the correct classification of some sample videos. \n",
    "Three videos are used for tennis, weight lifting, and football. \n",
    "In the present approach, each frame is classified on its own. \n",
    "We take advantage of the temporal characteristic of the sequence by assigning the label with respect the average classification of the previous `size_averaging` number of frames. \n",
    "If `size_averaging=1`, no average is performed, resulting in the naive approach of classifying each frame of the video on its own. \n",
    "This might cause some class-flickering, as the user is invited to experiment by changing the `size_averaging` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict video\n",
    "from collections import deque\n",
    "from scipy import stats\n",
    "import errno\n",
    "import urllib.request\n",
    "\n",
    "# Video to classify: options are \"tennis\", \"lifting\", \"soccer\"\n",
    "args[\"input_video\"] = \"lifting\"\n",
    "\n",
    "model = federated_government.global_model._model\n",
    "\n",
    "if not os.path.exists(args[\"input_path\"]):\n",
    "    os.mkdir(args[\"input_path\"])\n",
    "    print(\"[INFO] downloading video for prediction ...\" )\n",
    "    videos = [\"tennis\", \"lifting\", \"soccer\"]\n",
    "    for i_video in videos:\n",
    "        url = \"https://sh-ia-data.s3-eu-west-1.amazonaws.com/third_parties_data/example_clips/\" + i_video + \".mp4\"\n",
    "        local_path = os.path.join(args[\"input_path\"], i_video) + \".mp4\"\n",
    "        urllib.request.urlretrieve(url, local_path)\n",
    "\n",
    "# initialize the image mean for mean subtraction along with the\n",
    "# predictions queue\n",
    "mean = np.array([123.68, 116.779, 103.939][::1], dtype=\"float32\")\n",
    "Q = deque(maxlen=args[\"size_averaging\"])\n",
    "\n",
    "# initialize the video stream, pointer to output video file, and\n",
    "# frame dimensions\n",
    "input_video_path = os.path.join(args[\"input_path\"], args[\"input_video\"]) + \".mp4\"\n",
    "output_video_path = os.path.join(args[\"output_path\"], args[\"input_video\"]) + \"_classified.avi\"\n",
    "if not os.path.isfile(input_video_path):\n",
    "    raise FileNotFoundError(\n",
    "        errno.ENOENT, os.strerror(errno.ENOENT), input_video_path)\n",
    "vs = cv2.VideoCapture(input_video_path)\n",
    "writer = None\n",
    "(W, H) = (None, None)\n",
    "\n",
    "# loop over frames from the video file stream\n",
    "print(\"[INFO] predicting video ...\" )\n",
    "while True:\n",
    "    # read the next frame from the file\n",
    "    (grabbed, frame) = vs.read()\n",
    "\n",
    "    # if the frame was not grabbed, then we have reached the end\n",
    "    # of the stream\n",
    "    if not grabbed:\n",
    "        break\n",
    "\n",
    "    # if the frame dimensions are empty, grab them\n",
    "    if W is None or H is None:\n",
    "        (H, W) = frame.shape[:2]\n",
    "\n",
    "    # clone the output frame, then convert it from BGR to RGB\n",
    "    # ordering, resize the frame to a fixed 224x224, and then\n",
    "    # perform mean subtraction\n",
    "    output = frame.copy()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = cv2.resize(frame, (224, 224)).astype(\"float32\")\n",
    "    frame -= mean\n",
    "\n",
    "    # make predictions on the frame and then update the predictions\n",
    "    # queue\n",
    "    preds = model.predict(np.expand_dims(frame, axis=0))\n",
    "    Q.append(preds)\n",
    "\n",
    "    # perform prediction averaging over the current history of\n",
    "    # previous predictions\n",
    "    results = np.array(Q).mean(axis=0)\n",
    "    i = np.argmax(results)\n",
    "    label = lb.classes_[i]\n",
    "\n",
    "    # draw the activity on the output frame\n",
    "    text = \"activity: {}\".format(label)\n",
    "    cv2.putText(output, text, (35, 50), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1.25, (0, 255, 0), 5)\n",
    "\n",
    "    # check if the video writer is None\n",
    "    if writer is None:\n",
    "        # initialize our video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        writer = cv2.VideoWriter(output_video_path, fourcc, 30,\n",
    "            (W, H), True)\n",
    "\n",
    "    # write the output frame to disk\n",
    "    writer.write(output)\n",
    "\n",
    "\n",
    "# release the file pointers\n",
    "print(\"[INFO] classified video saved at \" + output_video_path)\n",
    "writer.release()\n",
    "vs.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding remarks\n",
    "The goal of the present notebook is to train a neural network for video classification in the **federated context** using Sherpa FL framework. \n",
    "It is shown that the procedure is straightforward using the framework classes.\n",
    "In this experiment, the network is trained on images (not videos), thus not taking into account any spatial-temporal information. \n",
    "More advanced approaches can be used when training over videos (see e.g. [Karpathy et al., 2014](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf) and [Simonyan and Zisserman, 2014](https://arxiv.org/pdf/1406.2199.pdf)).\n",
    "However, the only difference with respect what exposed in the present notebook would consist in setting up and training the local model, while the straightforward creation of the federated experiment with Sherpa FL framework would be exactly the same."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SherpaFL_py37",
   "language": "python",
   "name": "sherpafl_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
